{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook that converts data in LeetCode SQL 50 to spark dataframe and solves the same question using spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/20 22:02:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pandas_to_spark_df(df: pd.DataFrame) -> None:\n",
    "    return spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1) 1757. Recyclable and Low Fat Products\n",
    "\n",
    "### Products Table\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| product_id  | int     |\n",
    "| low_fats    | enum    |\n",
    "| recyclable  | enum    |\n",
    "\n",
    "- `product_id` is the primary key (column with unique values) for this table.\n",
    "- `low_fats` is an ENUM (category) of type ('Y', 'N') where 'Y' means this product is low fat and 'N' means it is not.\n",
    "- `recyclable` is an ENUM (category) of types ('Y', 'N') where 'Y' means this product is recyclable and 'N' means it is not.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Write a solution to find the ids of products that are both low fat and recyclable. Return the result table in any order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Products Table:**\n",
    "\n",
    "| product_id | low_fats | recyclable |\n",
    "|------------|----------|------------|\n",
    "| 0          | Y        | N          |\n",
    "| 1          | Y        | Y          |\n",
    "| 2          | N        | Y          |\n",
    "| 3          | Y        | Y          |\n",
    "| 4          | N        | N          |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| product_id |\n",
    "|------------|\n",
    "| 1          |\n",
    "| 3          |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "Only products 1 and 3 are both low fat and recyclable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['0', 'Y', 'N'], ['1', 'Y', 'Y'], ['2', 'N', 'Y'], ['3', 'Y', 'Y'], ['4', 'N', 'N']]\n",
    "products = pd.DataFrame(data, columns=['product_id', 'low_fats', 'recyclable']).astype({'product_id':'int64', 'low_fats':'category', 'recyclable':'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_df = convert_pandas_to_spark_df(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+\n",
      "|product_id|low_fats|recyclable|\n",
      "+----------+--------+----------+\n",
      "|         0|       Y|         N|\n",
      "|         1|       Y|         Y|\n",
      "|         2|       N|         Y|\n",
      "|         3|       Y|         Y|\n",
      "|         4|       N|         N|\n",
      "+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = q1_df \\\n",
    "    .filter((q1_df['low_fats'] == 'Y') & (q1_df['recyclable'] == 'Y')) \\\n",
    "    .select('product_id')\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2) 584. Find Customer Referee\n",
    "\n",
    "### Customer Table\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| id          | int     |\n",
    "| name        | varchar |\n",
    "| referee_id  | int     |\n",
    "\n",
    "- In SQL, `id` is the primary key column for this table.\n",
    "- Each row of this table indicates the id of a customer, their name, and the id of the customer who referred them.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Find the names of the customers that are not referred by the customer with id = 2. Return the result table in any order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Customer Table:**\n",
    "\n",
    "| id | name | referee_id |\n",
    "|----|------|------------|\n",
    "| 1  | Will | null       |\n",
    "| 2  | Jane | null       |\n",
    "| 3  | Alex | 2          |\n",
    "| 4  | Bill | null       |\n",
    "| 5  | Zack | 1          |\n",
    "| 6  | Mark | 2          |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| name |\n",
    "|------|\n",
    "| Will |\n",
    "| Jane |\n",
    "| Bill |\n",
    "| Zack |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "Customers Will, Jane, Bill, and Zack are not referred by the customer with id = 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Will', None], [2, 'Jane', None], [3, 'Alex', 2], [4, 'Bill', None], [5, 'Zack', 1], [6, 'Mark', 2]]\n",
    "customer = pd.DataFrame(data, columns=['id', 'name', 'referee_id']).astype({'id':'Int64', 'name':'object', 'referee_id':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_df = spark.createDataFrame(customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|Will|\n",
      "|Jane|\n",
      "|Bill|\n",
      "|Zack|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = q2_df \\\n",
    "    .filter((q2_df['referee_id'] != 2) | (q2_df['referee_id'] == F.lit(None))) \\\n",
    "    .select('name')\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3) 595. Big Countries\n",
    "\n",
    "### World Table\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| name        | varchar |\n",
    "| continent   | varchar |\n",
    "| area        | int     |\n",
    "| population  | int     |\n",
    "| gdp         | bigint  |\n",
    "\n",
    "- `name` is the primary key (column with unique values) for this table.\n",
    "- Each row of this table gives information about the name of a country, the continent to which it belongs, its area, the population, and its GDP value.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "A country is big if:\n",
    "- It has an area of at least three million (i.e., 3000000 kmÂ²), or\n",
    "- It has a population of at least twenty-five million (i.e., 25000000).\n",
    "\n",
    "Write a solution to find the name, population, and area of the big countries. Return the result table in any order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**World Table:**\n",
    "\n",
    "| name        | continent | area    | population | gdp          |\n",
    "|-------------|-----------|---------|------------|--------------|\n",
    "| Afghanistan | Asia      | 652230  | 25500100   | 20343000000  |\n",
    "| Albania     | Europe    | 28748   | 2831741    | 12960000000  |\n",
    "| Algeria     | Africa    | 2381741 | 37100000   | 188681000000 |\n",
    "| Andorra     | Europe    | 468     | 78115      | 3712000000   |\n",
    "| Angola      | Africa    | 1246700 | 20609294   | 100990000000 |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| name        | population | area    |\n",
    "|-------------|------------|---------|\n",
    "| Afghanistan | 25500100   | 652230  |\n",
    "| Algeria     | 37100000   | 2381741 |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "Afghanistan has a population of 25,500,100, making it a big country. Algeria has a population of 37,100,000, also qualifying as a big country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['Afghanistan', 'Asia', 652230, 25500100, 20343000000], ['Albania', 'Europe', 28748, 2831741, 12960000000], ['Algeria', 'Africa', 2381741, 37100000, 188681000000], ['Andorra', 'Europe', 468, 78115, 3712000000], ['Angola', 'Africa', 1246700, 20609294, 100990000000]]\n",
    "world = pd.DataFrame(data, columns=['name', 'continent', 'area', 'population', 'gdp']).astype({'name':'object', 'continent':'object', 'area':'Int64', 'population':'Int64', 'gdp':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_df = spark.createDataFrame(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+\n",
      "|       name|population|   area|\n",
      "+-----------+----------+-------+\n",
      "|Afghanistan|  25500100| 652230|\n",
      "|    Algeria|  37100000|2381741|\n",
      "+-----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = q3_df \\\n",
    "    .filter((q3_df['area'] >= 3000000) | (q3_df['population'] >= 25000000)) \\\n",
    "    .select('name', 'population', 'area')\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4) 1148. Article Views I\n",
    "\n",
    "### Views Table\n",
    "\n",
    "| Column Name   | Type    |\n",
    "|---------------|---------|\n",
    "| article_id    | int     |\n",
    "| author_id     | int     |\n",
    "| viewer_id     | int     |\n",
    "| view_date     | date    |\n",
    "\n",
    "- There is no primary key (column with unique values) for this table; the table may have duplicate rows.\n",
    "- Each row of this table indicates that some viewer viewed an article (written by some author) on some date. \n",
    "- Note that equal `author_id` and `viewer_id` indicate the same person.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Write a solution to find all the authors that viewed at least one of their own articles. Return the result table sorted by id in ascending order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Views Table:**\n",
    "\n",
    "| article_id | author_id | viewer_id | view_date  |\n",
    "|------------|-----------|-----------|------------|\n",
    "| 1          | 3         | 5         | 2019-08-01 |\n",
    "| 1          | 3         | 6         | 2019-08-02 |\n",
    "| 2          | 7         | 7         | 2019-08-01 |\n",
    "| 2          | 7         | 6         | 2019-08-02 |\n",
    "| 4          | 7         | 1         | 2019-07-22 |\n",
    "| 3          | 4         | 4         | 2019-07-21 |\n",
    "| 3          | 4         | 4         | 2019-07-21 |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| id   |\n",
    "|------|\n",
    "| 4    |\n",
    "| 7    |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "Author 4 viewed their own article, and author 7 also viewed at least one of their own articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 3, 5, '2019-08-01'], [1, 3, 6, '2019-08-02'], [2, 7, 7, '2019-08-01'], [2, 7, 6, '2019-08-02'], [4, 7, 1, '2019-07-22'], [3, 4, 4, '2019-07-21'], [3, 4, 4, '2019-07-21']]\n",
    "views = pd.DataFrame(data, columns=['article_id', 'author_id', 'viewer_id', 'view_date']).astype({'article_id':'Int64', 'author_id':'Int64', 'viewer_id':'Int64', 'view_date':'datetime64[ns]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4_df = spark.createDataFrame(views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  7|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = q4_df \\\n",
    "    .filter(q4_df['author_id'] == q4_df['viewer_id']) \\\n",
    "    .selectExpr('author_id as id') \\\n",
    "    .distinct()\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5) 1683. Invalid Tweets\n",
    "\n",
    "### Tweets Table\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| tweet_id    | int     |\n",
    "| content     | varchar |\n",
    "\n",
    "- `tweet_id` is the primary key (column with unique values) for this table.\n",
    "- This table contains all the tweets in a social media app.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Write a solution to find the IDs of the invalid tweets. The tweet is invalid if the number of characters used in the content of the tweet is strictly greater than 15. Return the result table in any order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Tweets Table:**\n",
    "\n",
    "| tweet_id | content                           |\n",
    "|----------|-----------------------------------|\n",
    "| 1        | Let us Code                       |\n",
    "| 2        | More than fifteen chars are here! |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| tweet_id |\n",
    "|----------|\n",
    "| 2        |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- Tweet 1 has length = 11. It is a valid tweet.\n",
    "- Tweet 2 has length = 33. It is an invalid tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Let us Code'], [2, 'More than fifteen chars are here!']]\n",
    "tweets = pd.DataFrame(data, columns=['tweet_id', 'content']).astype({'tweet_id':'Int64', 'content':'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5_df = spark.createDataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tweet_id|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = q5_df.filter(F.length(q5_df['content']) > 15).select('tweet_id')\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6) 1378. Replace Employee ID With The Unique Identifier\n",
    "\n",
    "### Employees Table\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| id          | int     |\n",
    "| name        | varchar |\n",
    "\n",
    "- `id` is the primary key (column with unique values) for this table.\n",
    "- Each row of this table contains the `id` and the name of an employee in a company.\n",
    "\n",
    "### EmployeeUNI Table\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| id          | int     |\n",
    "| unique_id   | int     |\n",
    "\n",
    "- `(id, unique_id)` is the primary key (combination of columns with unique values) for this table.\n",
    "- Each row of this table contains the `id` and the corresponding unique id of an employee in the company.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Write a solution to show the unique ID of each user. If a user does not have a unique ID, replace it with `null`. Return the result table in any order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Employees Table:**\n",
    "\n",
    "| id | name     |\n",
    "|----|----------|\n",
    "| 1  | Alice    |\n",
    "| 7  | Bob      |\n",
    "| 11 | Meir     |\n",
    "| 90 | Winston  |\n",
    "| 3  | Jonathan |\n",
    "\n",
    "**EmployeeUNI Table:**\n",
    "\n",
    "| id | unique_id |\n",
    "|----|-----------|\n",
    "| 3  | 1         |\n",
    "| 11 | 2         |\n",
    "| 90 | 3         |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| unique_id | name     |\n",
    "|-----------|----------|\n",
    "| null      | Alice    |\n",
    "| null      | Bob      |\n",
    "| 2         | Meir     |\n",
    "| 3         | Winston  |\n",
    "| 1         | Jonathan |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- Alice and Bob do not have a unique ID; we will show `null` instead.\n",
    "- The unique ID of Meir is 2.\n",
    "- The unique ID of Winston is 3.\n",
    "- The unique ID of Jonathan is 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Alice'], [7, 'Bob'], [11, 'Meir'], [90, 'Winston'], [3, 'Jonathan']]\n",
    "employees = pd.DataFrame(data, columns=['id', 'name']).astype({'id':'int64', 'name':'object'})\n",
    "data = [[3, 1], [11, 2], [90, 3]]\n",
    "employee_uni = pd.DataFrame(data, columns=['id', 'unique_id']).astype({'id':'int64', 'unique_id':'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = spark.createDataFrame(employees)\n",
    "emp_uni = spark.createDataFrame(employee_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|unique_id|    name|\n",
      "+---------+--------+\n",
      "|     NULL|   Alice|\n",
      "|     NULL|     Bob|\n",
      "|        2|    Meir|\n",
      "|        3| Winston|\n",
      "|        1|Jonathan|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = emp_df \\\n",
    "    .join(emp_uni, on='id', how='left') \\\n",
    "    .select('unique_id', 'name')\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7) 1068. Product Sales Analysis I\n",
    "\n",
    "### Sales Table\n",
    "\n",
    "| Column Name | Type  |\n",
    "|-------------|-------|\n",
    "| sale_id     | int   |\n",
    "| product_id  | int   |\n",
    "| year        | int   |\n",
    "| quantity    | int   |\n",
    "| price       | int   |\n",
    "\n",
    "- `(sale_id, year)` is the primary key (combination of columns with unique values) of this table.\n",
    "- `product_id` is a foreign key (reference column) to the Product table.\n",
    "- Each row of this table shows a sale on the product `product_id` in a certain year. Note that the price is per unit.\n",
    "\n",
    "### Product Table\n",
    "\n",
    "| Column Name  | Type    |\n",
    "|--------------|---------|\n",
    "| product_id   | int     |\n",
    "| product_name | varchar |\n",
    "\n",
    "- `product_id` is the primary key (column with unique values) of this table.\n",
    "- Each row of this table indicates the product name of each product.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Write a solution to report the `product_name`, `year`, and `price` for each `sale_id` in the Sales table. Return the resulting table in any order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Sales Table:**\n",
    "\n",
    "| sale_id | product_id | year | quantity | price |\n",
    "|---------|------------|------|----------|-------|\n",
    "| 1       | 100        | 2008 | 10       | 5000  |\n",
    "| 2       | 100        | 2009 | 12       | 5000  |\n",
    "| 7       | 200        | 2011 | 15       | 9000  |\n",
    "\n",
    "**Product Table:**\n",
    "\n",
    "| product_id | product_name |\n",
    "|------------|--------------|\n",
    "| 100        | Nokia        |\n",
    "| 200        | Apple        |\n",
    "| 300        | Samsung      |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| product_name | year  | price |\n",
    "|--------------|-------|-------|\n",
    "| Nokia        | 2008  | 5000  |\n",
    "| Nokia        | 2009  | 5000  |\n",
    "| Apple        | 2011  | 9000  |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- From `sale_id = 1`, we can conclude that Nokia was sold for 5000 in the year 2008.\n",
    "- From `sale_id = 2`, we can conclude that Nokia was sold for 5000 in the year 2009.\n",
    "- From `sale_id = 7`, we can conclude that Apple was sold for 9000 in the year 2011.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 100, 2008, 10, 5000], [2, 100, 2009, 12, 5000], [7, 200, 2011, 15, 9000]]\n",
    "sales = pd.DataFrame(data, columns=['sale_id', 'product_id', 'year', 'quantity', 'price']).astype({'sale_id':'Int64', 'product_id':'Int64', 'year':'Int64', 'quantity':'Int64', 'price':'Int64'})\n",
    "data = [[100, 'Nokia'], [200, 'Apple'], [300, 'Samsung']]\n",
    "product = pd.DataFrame(data, columns=['product_id', 'product_name']).astype({'product_id':'Int64', 'product_name':'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.createDataFrame(sales)\n",
    "product_df = spark.createDataFrame(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+\n",
      "|product_name|year|price|\n",
      "+------------+----+-----+\n",
      "|       Nokia|2008| 5000|\n",
      "|       Nokia|2009| 5000|\n",
      "|       Apple|2011| 9000|\n",
      "+------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = sales_df \\\n",
    "    .join(product_df, on='product_id') \\\n",
    "    .select('product_name', 'year', 'price')\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
