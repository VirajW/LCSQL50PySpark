{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook that converts data in LeetCode SQL 50 to spark dataframe and solves the same question using spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/20 22:02:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1) 1757. Recyclable and Low Fat Products\n",
    "\n",
    "### Products Table\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| product_id  | int     |\n",
    "| low_fats    | enum    |\n",
    "| recyclable  | enum    |\n",
    "\n",
    "- `product_id` is the primary key (column with unique values) for this table.\n",
    "- `low_fats` is an ENUM (category) of type ('Y', 'N') where 'Y' means this product is low fat and 'N' means it is not.\n",
    "- `recyclable` is an ENUM (category) of types ('Y', 'N') where 'Y' means this product is recyclable and 'N' means it is not.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Write a solution to find the ids of products that are both low fat and recyclable. Return the result table in any order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Products Table:**\n",
    "\n",
    "| product_id | low_fats | recyclable |\n",
    "|------------|----------|------------|\n",
    "| 0          | Y        | N          |\n",
    "| 1          | Y        | Y          |\n",
    "| 2          | N        | Y          |\n",
    "| 3          | Y        | Y          |\n",
    "| 4          | N        | N          |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| product_id |\n",
    "|------------|\n",
    "| 1          |\n",
    "| 3          |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "Only products 1 and 3 are both low fat and recyclable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['0', 'Y', 'N'], ['1', 'Y', 'Y'], ['2', 'N', 'Y'], ['3', 'Y', 'Y'], ['4', 'N', 'N']]\n",
    "products = pd.DataFrame(data, columns=['product_id', 'low_fats', 'recyclable']).astype({'product_id':'int64', 'low_fats':'category', 'recyclable':'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_df = spark.createDataFrame(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+\n",
      "|product_id|low_fats|recyclable|\n",
      "+----------+--------+----------+\n",
      "|         0|       Y|         N|\n",
      "|         1|       Y|         Y|\n",
      "|         2|       N|         Y|\n",
      "|         3|       Y|         Y|\n",
      "|         4|       N|         N|\n",
      "+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = q1_df \\\n",
    "    .filter((q1_df['low_fats'] == 'Y') & (q1_df['recyclable'] == 'Y')) \\\n",
    "    .select('product_id')\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2) 584. Find Customer Referee\n",
    "\n",
    "### Customer Table\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| id          | int     |\n",
    "| name        | varchar |\n",
    "| referee_id  | int     |\n",
    "\n",
    "- In SQL, `id` is the primary key column for this table.\n",
    "- Each row of this table indicates the id of a customer, their name, and the id of the customer who referred them.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Find the names of the customers that are not referred by the customer with id = 2. Return the result table in any order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Customer Table:**\n",
    "\n",
    "| id | name | referee_id |\n",
    "|----|------|------------|\n",
    "| 1  | Will | null       |\n",
    "| 2  | Jane | null       |\n",
    "| 3  | Alex | 2          |\n",
    "| 4  | Bill | null       |\n",
    "| 5  | Zack | 1          |\n",
    "| 6  | Mark | 2          |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| name |\n",
    "|------|\n",
    "| Will |\n",
    "| Jane |\n",
    "| Bill |\n",
    "| Zack |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "Customers Will, Jane, Bill, and Zack are not referred by the customer with id = 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Will', None], [2, 'Jane', None], [3, 'Alex', 2], [4, 'Bill', None], [5, 'Zack', 1], [6, 'Mark', 2]]\n",
    "customer = pd.DataFrame(data, columns=['id', 'name', 'referee_id']).astype({'id':'Int64', 'name':'object', 'referee_id':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_df = spark.createDataFrame(customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|Will|\n",
      "|Jane|\n",
      "|Bill|\n",
      "|Zack|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = q2_df \\\n",
    "    .filter((q2_df['referee_id'] != 2) | (q2_df['referee_id'] == F.lit(None))) \\\n",
    "    .select('name')\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3) 595. Big Countries\n",
    "\n",
    "### World Table\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| name        | varchar |\n",
    "| continent   | varchar |\n",
    "| area        | int     |\n",
    "| population  | int     |\n",
    "| gdp         | bigint  |\n",
    "\n",
    "- `name` is the primary key (column with unique values) for this table.\n",
    "- Each row of this table gives information about the name of a country, the continent to which it belongs, its area, the population, and its GDP value.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "A country is big if:\n",
    "- It has an area of at least three million (i.e., 3000000 kmÂ²), or\n",
    "- It has a population of at least twenty-five million (i.e., 25000000).\n",
    "\n",
    "Write a solution to find the name, population, and area of the big countries. Return the result table in any order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**World Table:**\n",
    "\n",
    "| name        | continent | area    | population | gdp          |\n",
    "|-------------|-----------|---------|------------|--------------|\n",
    "| Afghanistan | Asia      | 652230  | 25500100   | 20343000000  |\n",
    "| Albania     | Europe    | 28748   | 2831741    | 12960000000  |\n",
    "| Algeria     | Africa    | 2381741 | 37100000   | 188681000000 |\n",
    "| Andorra     | Europe    | 468     | 78115      | 3712000000   |\n",
    "| Angola      | Africa    | 1246700 | 20609294   | 100990000000 |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| name        | population | area    |\n",
    "|-------------|------------|---------|\n",
    "| Afghanistan | 25500100   | 652230  |\n",
    "| Algeria     | 37100000   | 2381741 |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "Afghanistan has a population of 25,500,100, making it a big country. Algeria has a population of 37,100,000, also qualifying as a big country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['Afghanistan', 'Asia', 652230, 25500100, 20343000000], ['Albania', 'Europe', 28748, 2831741, 12960000000], ['Algeria', 'Africa', 2381741, 37100000, 188681000000], ['Andorra', 'Europe', 468, 78115, 3712000000], ['Angola', 'Africa', 1246700, 20609294, 100990000000]]\n",
    "world = pd.DataFrame(data, columns=['name', 'continent', 'area', 'population', 'gdp']).astype({'name':'object', 'continent':'object', 'area':'Int64', 'population':'Int64', 'gdp':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_df = spark.createDataFrame(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+\n",
      "|       name|population|   area|\n",
      "+-----------+----------+-------+\n",
      "|Afghanistan|  25500100| 652230|\n",
      "|    Algeria|  37100000|2381741|\n",
      "+-----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = q3_df \\\n",
    "    .filter((q3_df['area'] >= 3000000) | (q3_df['population'] >= 25000000)) \\\n",
    "    .select('name', 'population', 'area')\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4) 1148. Article Views I\n",
    "\n",
    "### Views Table\n",
    "\n",
    "| Column Name   | Type    |\n",
    "|---------------|---------|\n",
    "| article_id    | int     |\n",
    "| author_id     | int     |\n",
    "| viewer_id     | int     |\n",
    "| view_date     | date    |\n",
    "\n",
    "- There is no primary key (column with unique values) for this table; the table may have duplicate rows.\n",
    "- Each row of this table indicates that some viewer viewed an article (written by some author) on some date. \n",
    "- Note that equal `author_id` and `viewer_id` indicate the same person.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Write a solution to find all the authors that viewed at least one of their own articles. Return the result table sorted by id in ascending order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Views Table:**\n",
    "\n",
    "| article_id | author_id | viewer_id | view_date  |\n",
    "|------------|-----------|-----------|------------|\n",
    "| 1          | 3         | 5         | 2019-08-01 |\n",
    "| 1          | 3         | 6         | 2019-08-02 |\n",
    "| 2          | 7         | 7         | 2019-08-01 |\n",
    "| 2          | 7         | 6         | 2019-08-02 |\n",
    "| 4          | 7         | 1         | 2019-07-22 |\n",
    "| 3          | 4         | 4         | 2019-07-21 |\n",
    "| 3          | 4         | 4         | 2019-07-21 |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| id   |\n",
    "|------|\n",
    "| 4    |\n",
    "| 7    |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "Author 4 viewed their own article, and author 7 also viewed at least one of their own articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 3, 5, '2019-08-01'], [1, 3, 6, '2019-08-02'], [2, 7, 7, '2019-08-01'], [2, 7, 6, '2019-08-02'], [4, 7, 1, '2019-07-22'], [3, 4, 4, '2019-07-21'], [3, 4, 4, '2019-07-21']]\n",
    "views = pd.DataFrame(data, columns=['article_id', 'author_id', 'viewer_id', 'view_date']).astype({'article_id':'Int64', 'author_id':'Int64', 'viewer_id':'Int64', 'view_date':'datetime64[ns]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4_df = spark.createDataFrame(views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  7|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = q4_df \\\n",
    "    .filter(q4_df['author_id'] == q4_df['viewer_id']) \\\n",
    "    .selectExpr('author_id as id') \\\n",
    "    .distinct()\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5) 1683. Invalid Tweets\n",
    "\n",
    "### Tweets Table\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| tweet_id    | int     |\n",
    "| content     | varchar |\n",
    "\n",
    "- `tweet_id` is the primary key (column with unique values) for this table.\n",
    "- This table contains all the tweets in a social media app.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Write a solution to find the IDs of the invalid tweets. The tweet is invalid if the number of characters used in the content of the tweet is strictly greater than 15. Return the result table in any order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Tweets Table:**\n",
    "\n",
    "| tweet_id | content                           |\n",
    "|----------|-----------------------------------|\n",
    "| 1        | Let us Code                       |\n",
    "| 2        | More than fifteen chars are here! |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| tweet_id |\n",
    "|----------|\n",
    "| 2        |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- Tweet 1 has length = 11. It is a valid tweet.\n",
    "- Tweet 2 has length = 33. It is an invalid tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Let us Code'], [2, 'More than fifteen chars are here!']]\n",
    "tweets = pd.DataFrame(data, columns=['tweet_id', 'content']).astype({'tweet_id':'Int64', 'content':'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5_df = spark.createDataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tweet_id|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = q5_df.filter(F.length(q5_df['content']) > 15).select('tweet_id')\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6) 1378. Replace Employee ID With The Unique Identifier\n",
    "\n",
    "### Employees Table\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| id          | int     |\n",
    "| name        | varchar |\n",
    "\n",
    "- `id` is the primary key (column with unique values) for this table.\n",
    "- Each row of this table contains the `id` and the name of an employee in a company.\n",
    "\n",
    "### EmployeeUNI Table\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| id          | int     |\n",
    "| unique_id   | int     |\n",
    "\n",
    "- `(id, unique_id)` is the primary key (combination of columns with unique values) for this table.\n",
    "- Each row of this table contains the `id` and the corresponding unique id of an employee in the company.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Write a solution to show the unique ID of each user. If a user does not have a unique ID, replace it with `null`. Return the result table in any order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Employees Table:**\n",
    "\n",
    "| id | name     |\n",
    "|----|----------|\n",
    "| 1  | Alice    |\n",
    "| 7  | Bob      |\n",
    "| 11 | Meir     |\n",
    "| 90 | Winston  |\n",
    "| 3  | Jonathan |\n",
    "\n",
    "**EmployeeUNI Table:**\n",
    "\n",
    "| id | unique_id |\n",
    "|----|-----------|\n",
    "| 3  | 1         |\n",
    "| 11 | 2         |\n",
    "| 90 | 3         |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| unique_id | name     |\n",
    "|-----------|----------|\n",
    "| null      | Alice    |\n",
    "| null      | Bob      |\n",
    "| 2         | Meir     |\n",
    "| 3         | Winston  |\n",
    "| 1         | Jonathan |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- Alice and Bob do not have a unique ID; we will show `null` instead.\n",
    "- The unique ID of Meir is 2.\n",
    "- The unique ID of Winston is 3.\n",
    "- The unique ID of Jonathan is 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Alice'], [7, 'Bob'], [11, 'Meir'], [90, 'Winston'], [3, 'Jonathan']]\n",
    "employees = pd.DataFrame(data, columns=['id', 'name']).astype({'id':'int64', 'name':'object'})\n",
    "data = [[3, 1], [11, 2], [90, 3]]\n",
    "employee_uni = pd.DataFrame(data, columns=['id', 'unique_id']).astype({'id':'int64', 'unique_id':'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = spark.createDataFrame(employees)\n",
    "emp_uni = spark.createDataFrame(employee_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|unique_id|    name|\n",
      "+---------+--------+\n",
      "|     NULL|   Alice|\n",
      "|     NULL|     Bob|\n",
      "|        2|    Meir|\n",
      "|        3| Winston|\n",
      "|        1|Jonathan|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = emp_df \\\n",
    "    .join(emp_uni, on='id', how='left') \\\n",
    "    .select('unique_id', 'name')\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7) 1068. Product Sales Analysis I\n",
    "\n",
    "### Sales Table\n",
    "\n",
    "| Column Name | Type  |\n",
    "|-------------|-------|\n",
    "| sale_id     | int   |\n",
    "| product_id  | int   |\n",
    "| year        | int   |\n",
    "| quantity    | int   |\n",
    "| price       | int   |\n",
    "\n",
    "- `(sale_id, year)` is the primary key (combination of columns with unique values) of this table.\n",
    "- `product_id` is a foreign key (reference column) to the Product table.\n",
    "- Each row of this table shows a sale on the product `product_id` in a certain year. Note that the price is per unit.\n",
    "\n",
    "### Product Table\n",
    "\n",
    "| Column Name  | Type    |\n",
    "|--------------|---------|\n",
    "| product_id   | int     |\n",
    "| product_name | varchar |\n",
    "\n",
    "- `product_id` is the primary key (column with unique values) of this table.\n",
    "- Each row of this table indicates the product name of each product.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Write a solution to report the `product_name`, `year`, and `price` for each `sale_id` in the Sales table. Return the resulting table in any order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Sales Table:**\n",
    "\n",
    "| sale_id | product_id | year | quantity | price |\n",
    "|---------|------------|------|----------|-------|\n",
    "| 1       | 100        | 2008 | 10       | 5000  |\n",
    "| 2       | 100        | 2009 | 12       | 5000  |\n",
    "| 7       | 200        | 2011 | 15       | 9000  |\n",
    "\n",
    "**Product Table:**\n",
    "\n",
    "| product_id | product_name |\n",
    "|------------|--------------|\n",
    "| 100        | Nokia        |\n",
    "| 200        | Apple        |\n",
    "| 300        | Samsung      |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| product_name | year  | price |\n",
    "|--------------|-------|-------|\n",
    "| Nokia        | 2008  | 5000  |\n",
    "| Nokia        | 2009  | 5000  |\n",
    "| Apple        | 2011  | 9000  |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- From `sale_id = 1`, we can conclude that Nokia was sold for 5000 in the year 2008.\n",
    "- From `sale_id = 2`, we can conclude that Nokia was sold for 5000 in the year 2009.\n",
    "- From `sale_id = 7`, we can conclude that Apple was sold for 9000 in the year 2011.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 100, 2008, 10, 5000], [2, 100, 2009, 12, 5000], [7, 200, 2011, 15, 9000]]\n",
    "sales = pd.DataFrame(data, columns=['sale_id', 'product_id', 'year', 'quantity', 'price']).astype({'sale_id':'Int64', 'product_id':'Int64', 'year':'Int64', 'quantity':'Int64', 'price':'Int64'})\n",
    "data = [[100, 'Nokia'], [200, 'Apple'], [300, 'Samsung']]\n",
    "product = pd.DataFrame(data, columns=['product_id', 'product_name']).astype({'product_id':'Int64', 'product_name':'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.createDataFrame(sales)\n",
    "product_df = spark.createDataFrame(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+\n",
      "|product_name|year|price|\n",
      "+------------+----+-----+\n",
      "|       Nokia|2008| 5000|\n",
      "|       Nokia|2009| 5000|\n",
      "|       Apple|2011| 9000|\n",
      "+------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = sales_df \\\n",
    "    .join(product_df, on='product_id') \\\n",
    "    .select('product_name', 'year', 'price')\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8) 1581. Customer Who Visited but Did Not Make Any Transactions\n",
    "\n",
    "### Visits Table\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| visit_id    | int     |\n",
    "| customer_id | int     |\n",
    "\n",
    "- `visit_id` is the column with unique values for this table.\n",
    "- This table contains information about the customers who visited the mall.\n",
    "\n",
    "### Transactions Table\n",
    "\n",
    "| Column Name    | Type    |\n",
    "|----------------|---------|\n",
    "| transaction_id | int     |\n",
    "| visit_id       | int     |\n",
    "| amount         | int     |\n",
    "\n",
    "- `transaction_id` is the column with unique values for this table.\n",
    "- This table contains information about the transactions made during the `visit_id`.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Write a solution to find the IDs of the users who visited without making any transactions and the number of times they made these types of visits. Return the result table sorted in any order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Visits Table:**\n",
    "\n",
    "| visit_id | customer_id |\n",
    "|----------|-------------|\n",
    "| 1        | 23          |\n",
    "| 2        | 9           |\n",
    "| 4        | 30          |\n",
    "| 5        | 54          |\n",
    "| 6        | 96          |\n",
    "| 7        | 54          |\n",
    "| 8        | 54          |\n",
    "\n",
    "**Transactions Table:**\n",
    "\n",
    "| transaction_id | visit_id | amount |\n",
    "|----------------|----------|--------|\n",
    "| 2              | 5        | 310    |\n",
    "| 3              | 5        | 300    |\n",
    "| 9              | 5        | 200    |\n",
    "| 12             | 1        | 910    |\n",
    "| 13             | 2        | 970    |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| customer_id | count_no_trans |\n",
    "|-------------|----------------|\n",
    "| 54          | 2              |\n",
    "| 30          | 1              |\n",
    "| 96          | 1              |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- Customer with id = 23 visited the mall once and made one transaction during the visit with id = 12.\n",
    "- Customer with id = 9 visited the mall once and made one transaction during the visit with id = 13.\n",
    "- Customer with id = 30 visited the mall once and did not make any transactions.\n",
    "- Customer with id = 54 visited the mall three times. During 2 visits they did not make any transactions, and during one visit they made 3 transactions.\n",
    "- Customer with id = 96 visited the mall once and did not make any transactions.\n",
    "\n",
    "As we can see, users with IDs 30 and 96 visited the mall one time without making any transactions. Also, user 54 visited the mall twice and did not make any transactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 23], [2, 9], [4, 30], [5, 54], [6, 96], [7, 54], [8, 54]]\n",
    "visits = pd.DataFrame(data, columns=['visit_id', 'customer_id']).astype({'visit_id':'Int64', 'customer_id':'Int64'})\n",
    "data = [[2, 5, 310], [3, 5, 300], [9, 5, 200], [12, 1, 910], [13, 2, 970]]\n",
    "transactions = pd.DataFrame(data, columns=['transaction_id', 'visit_id', 'amount']).astype({'transaction_id':'Int64', 'visit_id':'Int64', 'amount':'Int64'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_df = spark.createDataFrame(visits)\n",
    "trans_df = spark.createDataFrame(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|         54|    2|\n",
      "|         96|    1|\n",
      "|         30|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = visits_df \\\n",
    "    .join(trans_df, on='visit_id', how='left')\n",
    "\n",
    "res = res \\\n",
    "    .filter(res['amount'].isNull()) \\\n",
    "    .groupby('customer_id').count()\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9) 197. Rising Temperature\n",
    "\n",
    "### Weather Table\n",
    "\n",
    "| Column Name   | Type    |\n",
    "|---------------|---------|\n",
    "| id            | int     |\n",
    "| recordDate    | date    |\n",
    "| temperature   | int     |\n",
    "\n",
    "- `id` is the column with unique values for this table.\n",
    "- There are no different rows with the same `recordDate`.\n",
    "- This table contains information about the temperature on a certain day.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Write a solution to find all dates' id with higher temperatures compared to its previous dates (yesterday). Return the result table in any order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Weather Table:**\n",
    "\n",
    "| id | recordDate | temperature |\n",
    "|----|------------|-------------|\n",
    "| 1  | 2015-01-01 | 10          |\n",
    "| 2  | 2015-01-02 | 25          |\n",
    "| 3  | 2015-01-03 | 20          |\n",
    "| 4  | 2015-01-04 | 30          |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| id |\n",
    "|----|\n",
    "| 2  |\n",
    "| 4  |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- On 2015-01-02, the temperature was higher than the previous day (10 -> 25).\n",
    "- On 2015-01-04, the temperature was higher than the previous day (20 -> 30).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, '2015-01-01', 10], [2, '2015-01-02', 25], [3, '2015-01-03', 20], [4, '2015-01-04', 30]]\n",
    "weather = pd.DataFrame(data, columns=['id', 'recordDate', 'temperature']).astype({'id':'Int64', 'recordDate':'datetime64[ns]', 'temperature':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = spark.createDataFrame(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 23:16:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/20 23:16:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/20 23:16:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/20 23:16:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/20 23:16:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy().orderBy('recordDate')\n",
    "\n",
    "res = weather_df.withColumns({'previous_date': F.lag(weather_df['recordDate']).over(window_spec),\n",
    "                              'previous_temp': F.lag(weather_df['temperature']).over(window_spec)}) \\\n",
    "\n",
    "res = res \\\n",
    "    .filter((res['previous_date'].isNotNull()) & \\\n",
    "            (F.datediff(res['recordDate'], res['previous_date']) == 1) & \\\n",
    "            (res['temperature'] > res['previous_temp'])) \\\n",
    "    .select('id')\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10) 1661. Average Time of Process per Machine\n",
    "\n",
    "### Table: Activity\n",
    "\n",
    "| Column Name    | Type    |\n",
    "|----------------|---------|\n",
    "| machine_id     | int     |\n",
    "| process_id     | int     |\n",
    "| activity_type  | enum    |\n",
    "| timestamp      | float   |\n",
    "\n",
    "- `(machine_id, process_id, activity_type)` is the primary key (combination of columns with unique values) of this table.\n",
    "- `machine_id` is the ID of a machine.\n",
    "- `process_id` is the ID of a process running on the machine with ID `machine_id`.\n",
    "- `activity_type` is an ENUM (category) of type ('start', 'end').\n",
    "- `timestamp` is a float representing the current time in seconds.\n",
    "\n",
    "The 'start' timestamp will always be before the 'end' timestamp for every `(machine_id, process_id)` pair. It is guaranteed that each `(machine_id, process_id)` pair has a 'start' and 'end' timestamp.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Write a solution to find the average time each machine takes to complete a process. The time to complete a process is the 'end' timestamp minus the 'start' timestamp. The average time is calculated by the total time to complete every process on the machine divided by the number of processes that were run. The resulting table should have the `machine_id` along with the average time as `processing_time`, rounded to 3 decimal places.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Activity Table:**\n",
    "\n",
    "| machine_id | process_id | activity_type | timestamp |\n",
    "|------------|------------|---------------|-----------|\n",
    "| 0          | 0          | start         | 0.712     |\n",
    "| 0          | 0          | end           | 1.520     |\n",
    "| 0          | 1          | start         | 3.140     |\n",
    "| 0          | 1          | end           | 4.120     |\n",
    "| 1          | 0          | start         | 0.550     |\n",
    "| 1          | 0          | end           | 1.550     |\n",
    "| 1          | 1          | start         | 0.430     |\n",
    "| 1          | 1          | end           | 1.420     |\n",
    "| 2          | 0          | start         | 4.100     |\n",
    "| 2          | 0          | end           | 4.512     |\n",
    "| 2          | 1          | start         | 2.500     |\n",
    "| 2          | 1          | end           | 5.000     |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| machine_id | processing_time |\n",
    "|------------|-----------------|\n",
    "| 0          | 0.894           |\n",
    "| 1          | 0.995           |\n",
    "| 2          | 1.456           |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- Machine 0's average time is ((1.520 - 0.712) + (4.120 - 3.140)) / 2 = 0.894\n",
    "- Machine 1's average time is ((1.550 - 0.550) + (1.420 - 0.430)) / 2 = 0.995\n",
    "- Machine 2's average time is ((4.512 - 4.100) + (5.000 - 2.500)) / 2 = 1.456\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0, 0, 'start', 0.712], [0, 0, 'end', 1.52], [0, 1, 'start', 3.14], [0, 1, 'end', 4.12], [1, 0, 'start', 0.55], [1, 0, 'end', 1.55], [1, 1, 'start', 0.43], [1, 1, 'end', 1.42], [2, 0, 'start', 4.1], [2, 0, 'end', 4.512], [2, 1, 'start', 2.5], [2, 1, 'end', 5]]\n",
    "activity = pd.DataFrame(data, columns=['machine_id', 'process_id', 'activity_type', 'timestamp']).astype({'machine_id':'Int64', 'process_id':'Int64', 'activity_type':'object', 'timestamp':'Float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = spark.createDataFrame(activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11) 577. Employee Bonus\n",
    "\n",
    "### Table: Employee\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| empId       | int     |\n",
    "| name        | varchar |\n",
    "| supervisor  | int     |\n",
    "| salary      | int     |\n",
    "\n",
    "- `empId` is the column with unique values for this table.\n",
    "- Each row of this table indicates the name and the ID of an employee in addition to their salary and the ID of their manager.\n",
    "\n",
    "### Table: Bonus\n",
    "\n",
    "| Column Name | Type |\n",
    "|-------------|------|\n",
    "| empId       | int  |\n",
    "| bonus       | int  |\n",
    "\n",
    "- `empId` is the column of unique values for this table.\n",
    "- `empId` is a foreign key (reference column) to `empId` from the Employee table.\n",
    "- Each row of this table contains the ID of an employee and their respective bonus.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Write a solution to report the name and bonus amount of each employee with a bonus less than 1000. Return the result table in any order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Employee Table:**\n",
    "\n",
    "| empId | name   | supervisor | salary |\n",
    "|-------|--------|------------|--------|\n",
    "| 3     | Brad   | null       | 4000   |\n",
    "| 1     | John   | 3          | 1000   |\n",
    "| 2     | Dan    | 3          | 2000   |\n",
    "| 4     | Thomas | 3          | 4000   |\n",
    "\n",
    "**Bonus Table:**\n",
    "\n",
    "| empId | bonus |\n",
    "|-------|-------|\n",
    "| 2     | 500   |\n",
    "| 4     | 2000  |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| name | bonus |\n",
    "|------|-------|\n",
    "| Brad | null  |\n",
    "| John | null  |\n",
    "| Dan  | 500   |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- Brad has no bonus recorded, so it shows as null.\n",
    "- John has no bonus recorded, so it shows as null.\n",
    "- Dan has a bonus of 500, which is less than 1000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[3, 'Brad', None, 4000], [1, 'John', 3, 1000], [2, 'Dan', 3, 2000], [4, 'Thomas', 3, 4000]]\n",
    "employee = pd.DataFrame(data, columns=['empId', 'name', 'supervisor', 'salary']).astype({'empId':'Int64', 'name':'object', 'supervisor':'Int64', 'salary':'Int64'})\n",
    "data = [[2, 500], [4, 2000]]\n",
    "bonus = pd.DataFrame(data, columns=['empId', 'bonus']).astype({'empId':'Int64', 'bonus':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = spark.createDataFrame(employee)\n",
    "bonus_df = spark.createDataFrame(bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|bonus|\n",
      "+----+-----+\n",
      "|Brad| NULL|\n",
      "|John| NULL|\n",
      "| Dan|  500|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = employee_df \\\n",
    "    .join(bonus_df, how='left', on='empId')\n",
    "\n",
    "res = res \\\n",
    "    .filter((res['bonus'].isNull()) | (res['bonus'] < 1000)) \\\n",
    "    .select('name', 'bonus')\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q12) 1280. Students and Examinations\n",
    "\n",
    "### Table: Students\n",
    "\n",
    "| Column Name   | Type    |\n",
    "|---------------|---------|\n",
    "| student_id    | int     |\n",
    "| student_name  | varchar |\n",
    "\n",
    "- `student_id` is the primary key (column with unique values) for this table.\n",
    "- Each row of this table contains the ID and the name of one student in the school.\n",
    "\n",
    "### Table: Subjects\n",
    "\n",
    "| Column Name  | Type    |\n",
    "|--------------|---------|\n",
    "| subject_name | varchar |\n",
    "\n",
    "- `subject_name` is the primary key (column with unique values) for this table.\n",
    "- Each row of this table contains the name of one subject in the school.\n",
    "\n",
    "### Table: Examinations\n",
    "\n",
    "| Column Name  | Type    |\n",
    "|--------------|---------|\n",
    "| student_id   | int     |\n",
    "| subject_name | varchar |\n",
    "\n",
    "- There is no primary key (column with unique values) for this table. It may contain duplicates.\n",
    "- Each student from the Students table takes every course from the Subjects table.\n",
    "- Each row of this table indicates that a student with ID `student_id` attended the exam of `subject_name`.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Write a solution to find the number of times each student attended each exam. Return the result table ordered by `student_id` and `subject_name`.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Students Table:**\n",
    "\n",
    "| student_id | student_name |\n",
    "|------------|--------------|\n",
    "| 1          | Alice        |\n",
    "| 2          | Bob          |\n",
    "| 13         | John         |\n",
    "| 6          | Alex         |\n",
    "\n",
    "**Subjects Table:**\n",
    "\n",
    "| subject_name |\n",
    "|--------------|\n",
    "| Math         |\n",
    "| Physics      |\n",
    "| Programming  |\n",
    "\n",
    "**Examinations Table:**\n",
    "\n",
    "| student_id | subject_name |\n",
    "|------------|--------------|\n",
    "| 1          | Math         |\n",
    "| 1          | Physics      |\n",
    "| 1          | Programming  |\n",
    "| 2          | Programming  |\n",
    "| 1          | Physics      |\n",
    "| 1          | Math         |\n",
    "| 13         | Math         |\n",
    "| 13         | Programming  |\n",
    "| 13         | Physics      |\n",
    "| 2          | Math         |\n",
    "| 1          | Math         |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| student_id | student_name | subject_name | attended_exams |\n",
    "|------------|--------------|--------------|----------------|\n",
    "| 1          | Alice        | Math         | 3              |\n",
    "| 1          | Alice        | Physics      | 2              |\n",
    "| 1          | Alice        | Programming  | 1              |\n",
    "| 2          | Bob          | Math         | 1              |\n",
    "| 2          | Bob          | Physics      | 0              |\n",
    "| 2          | Bob          | Programming  | 1              |\n",
    "| 6          | Alex         | Math         | 0              |\n",
    "| 6          | Alex         | Physics      | 0              |\n",
    "| 6          | Alex         | Programming  | 0              |\n",
    "| 13         | John         | Math         | 1              |\n",
    "| 13         | John         | Physics      | 1              |\n",
    "| 13         | John         | Programming  | 1              |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- The result table contains all students and all subjects.\n",
    "- Alice attended the Math exam 3 times, the Physics exam 2 times, and the Programming exam 1 time.\n",
    "- Bob attended the Math exam 1 time, the Programming exam 1 time, and did not attend the Physics exam.\n",
    "- Alex did not attend any exams.\n",
    "- John attended the Math exam 1 time, the Physics exam 1 time, and the Programming exam 1 time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 'Alice'], [2, 'Bob'], [13, 'John'], [6, 'Alex']]\n",
    "students = pd.DataFrame(data, columns=['student_id', 'student_name']).astype({'student_id':'Int64', 'student_name':'object'})\n",
    "data = [['Math'], ['Physics'], ['Programming']]\n",
    "subjects = pd.DataFrame(data, columns=['subject_name']).astype({'subject_name':'object'})\n",
    "data = [[1, 'Math'], [1, 'Physics'], [1, 'Programming'], [2, 'Programming'], [1, 'Physics'], [1, 'Math'], [13, 'Math'], [13, 'Programming'], [13, 'Physics'], [2, 'Math'], [1, 'Math']]\n",
    "examinations = pd.DataFrame(data, columns=['student_id', 'subject_name']).astype({'student_id':'Int64', 'subject_name':'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_df = spark.createDataFrame(students)\n",
    "subjects_df = spark.createDataFrame(subjects)\n",
    "exam_df = spark.createDataFrame(examinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q13) 570. Managers with at Least 5 Direct Reports\n",
    "\n",
    "### Table: Employee\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| id          | int     |\n",
    "| name        | varchar |\n",
    "| department  | varchar |\n",
    "| managerId   | int     |\n",
    "\n",
    "- `id` is the primary key (column with unique values) for this table.\n",
    "- Each row of this table indicates the name of an employee, their department, and the id of their manager.\n",
    "- If `managerId` is null, then the employee does not have a manager.\n",
    "- No employee will be the manager of themselves.\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "Write a solution to find managers with at least five direct reports. Return the result table in any order.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Employee Table:**\n",
    "\n",
    "| id  | name  | department | managerId |\n",
    "|-----|-------|------------|-----------|\n",
    "| 101 | John  | A          | null      |\n",
    "| 102 | Dan   | A          | 101       |\n",
    "| 103 | James | A          | 101       |\n",
    "| 104 | Amy   | A          | 101       |\n",
    "| 105 | Anne  | A          | 101       |\n",
    "| 106 | Ron   | B          | 101       |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| name |\n",
    "|------|\n",
    "| John |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- In this example, John is the manager of Dan, James, Amy, Anne, and Ron, totaling five direct reports.\n",
    "- Therefore, the output is John, who meets the criteria of having at least five direct reports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[101, 'John', 'A', None], [102, 'Dan', 'A', 101], [103, 'James', 'A', 101], [104, 'Amy', 'A', 101], [105, 'Anne', 'A', 101], [106, 'Ron', 'B', 101]]\n",
    "employee = pd.DataFrame(data, columns=['id', 'name', 'department', 'managerId']).astype({'id':'Int64', 'name':'object', 'department':'object', 'managerId':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_df = spark.createDataFrame(employee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|John|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m_df = e_df \\\n",
    "    .groupby('managerId') \\\n",
    "    .count() \\\n",
    "    .dropna()\n",
    "\n",
    "res = e_df \\\n",
    "    .join(m_df, m_df['managerId'] == e_df['id']) \\\n",
    "    .select('name')\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q14) 1934. Confirmation Rate\n",
    "\n",
    "### Table: Signups\n",
    "\n",
    "| Column Name | Type     |\n",
    "|-------------|----------|\n",
    "| user_id     | int      |\n",
    "| time_stamp   | datetime |\n",
    "\n",
    "- `user_id` is the column of unique values for this table.\n",
    "- Each row contains information about the signup time for the user with ID `user_id`.\n",
    "\n",
    "### Table: Confirmations\n",
    "\n",
    "| Column Name | Type     |\n",
    "|-------------|----------|\n",
    "| user_id     | int      |\n",
    "| time_stamp   | datetime |\n",
    "| action      | ENUM     |\n",
    "\n",
    "- `(user_id, time_stamp)` is the primary key (combination of columns with unique values) for this table.\n",
    "- `user_id` is a foreign key (reference column) to the Signups table.\n",
    "- `action` is an ENUM (category) of the type ('confirmed', 'timeout').\n",
    "- Each row indicates that the user with ID `user_id` requested a confirmation message at `time_stamp` and that confirmation message was either confirmed ('confirmed') or expired without confirming ('timeout').\n",
    "\n",
    "### Solution Requirement\n",
    "\n",
    "The confirmation rate of a user is defined as the number of 'confirmed' messages divided by the total number of requested confirmation messages. If a user did not request any confirmation messages, the confirmation rate should be 0. Round the confirmation rate to two decimal places.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Input\n",
    "\n",
    "**Signups Table:**\n",
    "\n",
    "| user_id | time_stamp          |\n",
    "|---------|---------------------|\n",
    "| 3       | 2020-03-21 10:16:13 |\n",
    "| 7       | 2020-01-04 13:57:59 |\n",
    "| 2       | 2020-07-29 23:09:44 |\n",
    "| 6       | 2020-12-09 10:39:37 |\n",
    "\n",
    "**Confirmations Table:**\n",
    "\n",
    "| user_id | time_stamp          | action    |\n",
    "|---------|---------------------|-----------|\n",
    "| 3       | 2021-01-06 03:30:46 | timeout   |\n",
    "| 3       | 2021-07-14 14:00:00 | timeout   |\n",
    "| 7       | 2021-06-12 11:57:29 | confirmed |\n",
    "| 7       | 2021-06-13 12:58:28 | confirmed |\n",
    "| 7       | 2021-06-14 13:59:27 | confirmed |\n",
    "| 2       | 2021-01-22 00:00:00 | confirmed |\n",
    "| 2       | 2021-02-28 23:59:59 | timeout   |\n",
    "\n",
    "#### Output\n",
    "\n",
    "| user_id | confirmation_rate |\n",
    "|---------|-------------------|\n",
    "| 6       | 0.00              |\n",
    "| 3       | 0.00              |\n",
    "| 7       | 1.00              |\n",
    "| 2       | 0.50              |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- User 6 did not request any confirmation messages, so the confirmation rate is 0.\n",
    "- User 3 made 2 requests, both of which timed out, resulting in a confirmation rate of 0.\n",
    "- User 7 made 3 requests, all confirmed, giving a confirmation rate of 1.\n",
    "- User 2 made 2 requests (1 confirmed, 1 timed out), resulting in a confirmation rate of 0.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[3, '2020-03-21 10:16:13'], [7, '2020-01-04 13:57:59'], [2, '2020-07-29 23:09:44'], [6, '2020-12-09 10:39:37']]\n",
    "signups = pd.DataFrame(data, columns=['user_id', 'time_stamp']).astype({'user_id':'Int64', 'time_stamp':'datetime64[ns]'})\n",
    "data = [[3, '2021-01-06 03:30:46', 'timeout'], [3, '2021-07-14 14:00:00', 'timeout'], [7, '2021-06-12 11:57:29', 'confirmed'], [7, '2021-06-13 12:58:28', 'confirmed'], [7, '2021-06-14 13:59:27', 'confirmed'], [2, '2021-01-22 00:00:00', 'confirmed'], [2, '2021-02-28 23:59:59', 'timeout']]\n",
    "confirmations = pd.DataFrame(data, columns=['user_id', 'time_stamp', 'action']).astype({'user_id':'Int64', 'time_stamp':'datetime64[ns]', 'action':'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "signup_df = spark.createDataFrame(signups)\n",
    "confirm_df = spark.createDataFrame(confirmations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10, 12, 14 remaining to solve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
